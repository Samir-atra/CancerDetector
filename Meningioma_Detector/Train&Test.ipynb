{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "# ----------\n",
    "# All necessary libraries and modules are imported here.\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL.Image\n",
    "from PIL import ImageOps\n",
    "import PIL\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import tensorboard\n",
    "import IPython\n",
    "import sklearn\n",
    "import cv2\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dataset Loading\n",
    "# --------------------\n",
    "# This cell handles the loading of the training and testing datasets.\n",
    "# The paths are hardcoded, so they will need to be changed if running elsewhere.\n",
    "\n",
    "# Define the paths to the training and testing data.\n",
    "# IMPORTANT: These paths are specific to the original author's machine.\n",
    "data_path = pathlib.Path('/home/samer/Documents/Programming/AI50xIraq/Cancerdetection/archivecopy/Training/')\n",
    "data_path_test = pathlib.Path('/home/samer/Documents/Programming/AI50xIraq/Cancerdetection/archivecopy/Testing/')\n",
    "\n",
    "# Create the training and validation datasets from the training directory.\n",
    "dataset_path, dataset_path_val = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_path,\n",
    "    labels='inferred',\n",
    "    validation_split=0.2,  # 20% of the data will be used for validation.\n",
    "    subset='both',         # Returns both training and validation sets.\n",
    "    seed=1,                # Seed for reproducibility.\n",
    "    batch_size=5,\n",
    "    image_size=(180, 180), # Resize images to 180x180.\n",
    "    color_mode=\"grayscale\",# Convert images to grayscale.\n",
    "    shuffle=True)\n",
    "\n",
    "# Create the testing dataset from the testing directory.\n",
    "dataset_path_test = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_path_test,\n",
    "    labels='inferred',\n",
    "    seed=3,\n",
    "    batch_size=5,\n",
    "    image_size=(180, 180),\n",
    "    color_mode=\"grayscale\",\n",
    "    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Performance Optimization\n",
    "# ---------------------------\n",
    "# To improve performance, the datasets are cached and prefetched.\n",
    "# .cache() keeps the images in memory after they're loaded off disk during the first epoch.\n",
    "# .prefetch() overlaps data preprocessing and model execution while training.\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "dataset_path = dataset_path.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "dataset_path_val = dataset_path_val.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Exploration and Visualization\n",
    "# -------------------------------------\n",
    "# This cell is for exploring the dataset and visualizing some of the images.\n",
    "# Note: Some of this code is for debugging and exploration purposes.\n",
    "\n",
    "# Get the class names from the dataset.\n",
    "class_names = dataset_path.class_names\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Get the total number of images.\n",
    "data_dir = pathlib.Path(data_path)\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(f\"Total images: {image_count}\")\n",
    "\n",
    "# Visualize a few images from the training set.\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in dataset_path.take(1):\n",
    "   for i in range(2):\n",
    "       ax = plt.subplot(1, 2, i + 1)\n",
    "       plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "       plt.title(class_names[labels[i]])\n",
    "       plt.axis(\"off\")\n",
    "   plt.show()\n",
    "\n",
    "# Print the shape of the image and label batches for debugging.\n",
    "print(\"--- Batch Shapes ---\")\n",
    "for image_batch, labels_batch in dataset_path.take(1):\n",
    "   print(f\"Image batch shape: {image_batch.shape}\")\n",
    "   print(f\"Labels batch shape: {labels_batch.shape}\")\n",
    "\n",
    "# The following code seems to be for debugging and will cause an error\n",
    "# as `get_label_name` is not defined. It has been commented out.\n",
    "# image, label = next(iter(dataset_path.take(1)))\n",
    "# _ = plt.imshow(image.numpy().astype(\"uint8\"))\n",
    "# _ = plt.title(get_label_name(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Definition and Training\n",
    "# --------------------------------\n",
    "# This cell defines, compiles, and trains the Convolutional Neural Network (CNN).\n",
    "\n",
    "num_classes = 2 # Binary classification (e.g., 'meningioma' or 'notumor')\n",
    "\n",
    "# Define the model architecture using tf.keras.Sequential.\n",
    "model = tf.keras.Sequential([\n",
    "    # Input layers: Resize images and rescale pixel values to [0, 1].\n",
    "    tf.keras.layers.Resizing(60, 60),\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "\n",
    "    # Convolutional Block 1\n",
    "    tf.keras.layers.Conv2D(16, 3, activation='ELU', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2), # Dropout for regularization.\n",
    "\n",
    "    # Convolutional Block 2\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='ELU', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    # Convolutional Block 3\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='ELU', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    # Flatten the feature maps and feed into Dense layers.\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='ELU', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation='ELU', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    # Output layer with softmax activation for classification.\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy', 'mse'])\n",
    "\n",
    "# Set up TensorBoard for logging.\n",
    "# Note: The log directory is hardcoded.\n",
    "log_dir = \"/home/samer/Documents/Programming/AI50xIraq/Cancerdetection/TBLog/\" + datetime.datetime.now().strftime(\"%Y%M%D-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Set up ModelCheckpoint to save weights during training.\n",
    "# Note: The checkpoint path is hardcoded.\n",
    "checkpoint_path = \"/home/samer/Documents/Programming/AI50xIraq/Cancerdetection/Checkpoints/cp-{epoch:04d}.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    verbose=1)\n",
    "\n",
    "# Save the initial weights.\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "    dataset_path,\n",
    "    epochs=15,\n",
    "    validation_data=dataset_path_val,\n",
    "    callbacks=[\n",
    "        tensorboard_callback,\n",
    "        cp_callback\n",
    "    ])\n",
    "\n",
    "# Evaluate the model on the test set.\n",
    "print(\"\\nEvaluating model on the test dataset...\")\n",
    "model.evaluate(dataset_path_test, batch_size=5, verbose=2)\n",
    "\n",
    "# Print a summary of the model architecture.\n",
    "model.summary()\n",
    "\n",
    "# Save the entire model.\n",
    "# Note: The saving path is hardcoded.\n",
    "saving_path = pathlib.Path('/home/samer/Documents/Programming/AI50xIraq/Cancerdetection/SavedModel/')\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    saving_path,\n",
    "    overwrite=True,\n",
    "    save_format='tf')\n",
    "\n",
    "print(f\"\\nModel saved to {saving_path}\")\n",
    "\n",
    "# To load the model later:\n",
    "# loaded_model = tf.keras.models.load_model('/home/samer/Documents/Programming/AI50xIraq/Cancerdetection/SavedModel/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "- use the dataset to train, validate and test.               (done)\n",
    "- use dropout to avoid overfitting.                          (done)\n",
    "- save a copy to load when deploying.                        (done)\n",
    "- deploy with tensorflow.js. or TFX                          (predictionful)\n",
    "- add some augmentation to expand the dataset.               (done)\n",
    "- add more classes to work on the full dataset.              (done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record\n",
    "- ELU + sigmoid around 98 val 97\n",
    "- ELU + softmax around 97 val 98 dips though but best yet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4de0671d8054c88a8842951ed8f6115b974e2e5b39cbc2553be5a6678282e63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
